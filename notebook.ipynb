{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-fkLZ-I5S6E"
      },
      "outputs": [],
      "source": [
        "/content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynISHZpP7fga",
        "outputId": "1fd2b28a-a219-4a1a-ee54-00cba48a0f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Specify the input and output file paths\n",
        "input_txt_file = '/content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.txt'\n",
        "output_csv_file = '/content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.csv'\n",
        "\n",
        "# Open the input TXT file for reading\n",
        "with open(input_txt_file, 'r') as txt_file:\n",
        "    # Read the lines from the TXT file\n",
        "    lines = txt_file.readlines()\n",
        "\n",
        "# Split the lines into rows (assuming a specific format)\n",
        "# Modify this part according to your specific TXT file format\n",
        "data = [line.strip().split('\\t') for line in lines]\n",
        "\n",
        "# Open the output CSV file for writing\n",
        "with open(output_csv_file, 'w', newline='') as csv_file:\n",
        "    # Create a CSV writer\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    csv_writer.writerows(data)\n",
        "\n",
        "print(f'Conversion from {input_txt_file} to {output_csv_file} completed.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_YT5vwC7f9N",
        "outputId": "a9c3bc18-b3c2-43b9-d35e-d318dc754519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion from /content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.txt to /content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.csv completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.csv', sep=',')"
      ],
      "metadata": {
        "id": "o-AguN4U7pNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file with one column\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/icd-9dataset-final.csv'\n",
        "df = pd.read_csv(input_file, header=None, names=['icd code,Description,DiagnosisNotes'])\n",
        "\n",
        "# Split the single column into three columns by \",\"\n",
        "df[['icd code', 'Description', 'DiagnosisNotes']] = df['icd code,Description,DiagnosisNotes'].str.split(',', n=2, expand=True)\n",
        "\n",
        "# Drop the original 'Data' column\n",
        "df = df.drop(columns=['icd code,Description,DiagnosisNotes'])\n",
        "\n",
        "# Save the DataFrame to a new CSV file with three columns\n",
        "output_file = 'output.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f'CSV file with three columns saved as {output_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkAjNvVH7u8M",
        "outputId": "34df0ffd-970f-4674-f6b0-4529f3f72795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file with three columns saved as output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"output.csv\")\n",
        "df = df.iloc[1:]\n",
        "\n",
        "# Reset the index\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df= df.dropna(how='all')"
      ],
      "metadata": {
        "id": "1SUcFWbK7zzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by tokenizing, removing stopwords and non-alphanumeric tokens, and lemmatizing the tokens.\n",
        "\n",
        "    Args:\n",
        "        text: The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        A preprocessed string.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and non-alphanumeric tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Convert tokens back to a single string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "id": "AvFzN8Cf76Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the 'DiagnosisNotes' column exists\n",
        "if 'DiagnosisNotes' in df.columns:\n",
        "    # Apply preprocessing to the 'DiagnosisNotes' column\n",
        "    df['DiagnosisNotes'] = df['DiagnosisNotes'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "kJcmjmw-78JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the 'DiagnosisNotes' column exists\n",
        "if 'DiagnosisNotes' in data.columns:\n",
        "\n",
        "    # Transform the data into TF-IDF vectors\n",
        "    X_tfidf = tfidf_vectorizer.fit_transform(data['DiagnosisNotes'])\n",
        "\n",
        "    # Create the DiagnosisNotes column if it does not exist\n",
        "elif 'DiagnosisNotes' not in data.columns:\n",
        "    data['DiagnosisNotes'] = ''\n",
        "\n",
        "else:\n",
        "\n",
        "    # Handle the case where the 'DiagnosisNotes' column does not exist\n",
        "    print('The DiagnosisNotes column does not exist in the data DataFrame.')"
      ],
      "metadata": {
        "id": "qubeUsn679u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "\n",
        "# Transform the data into TF-IDF vectors\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['DiagnosisNotes'])"
      ],
      "metadata": {
        "id": "banD7y0y8UUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and non-alphanumeric tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Convert tokens back to a single string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n"
      ],
      "metadata": {
        "id": "Lio7hfU98Wu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the 'DiagnosisNotes' column\n",
        "df['DiagnosisNotes'] = df['DiagnosisNotes'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "hrniiZEk8YaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['DiagnosisNotes'])\n",
        "\n",
        "# Convert TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Add the 'icd code' and 'Description' columns back to the DataFrame\n",
        "tfidf_df[['icd code', 'Description']] = df[['icd code', 'Description']]\n",
        "\n",
        "# Display the preprocessed and vectorized DataFrame\n",
        "print(tfidf_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMr0Rf6W8Zx0",
        "outputId": "b94617e0-23c8-403b-cac6-497942db4b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   226  390   47  566  abdominal  abnormal  abnormality  abortion  abscess  \\\n",
            "0  0.0  0.0  0.0  0.0   0.000000       0.0          0.0       0.0      0.0   \n",
            "1  0.0  0.0  0.0  0.0   0.000000       0.0          0.0       0.0      0.0   \n",
            "2  0.0  0.0  0.0  0.0   0.296842       0.0          0.0       0.0      0.0   \n",
            "3  0.0  0.0  0.0  0.0   0.000000       0.0          0.0       0.0      0.0   \n",
            "4  0.0  0.0  0.0  0.0   0.228055       0.0          0.0       0.0      0.0   \n",
            "\n",
            "   absence  ...  wall  whether  withdrawal  within  without  wuchereria  year  \\\n",
            "0      0.0  ...   0.0      0.0         0.0     0.0      0.0         0.0   0.0   \n",
            "1      0.0  ...   0.0      0.0         0.0     0.0      0.0         0.0   0.0   \n",
            "2      0.0  ...   0.0      0.0         0.0     0.0      0.0         0.0   0.0   \n",
            "3      0.0  ...   0.0      0.0         0.0     0.0      0.0         0.0   0.0   \n",
            "4      0.0  ...   0.0      0.0         0.0     0.0      0.0         0.0   0.0   \n",
            "\n",
            "   zone  icd code                     Description  \n",
            "0   0.0       001                         Cholera  \n",
            "1   0.0     001.0          Due to Vibrio cholerae  \n",
            "2   0.0     001.1   Due to Vibrio cholerae el tor  \n",
            "3   0.0     001.9                     Unspecified  \n",
            "4   0.0       002  Typhoid and paratyphoid fevers  \n",
            "\n",
            "[5 rows x 1002 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the lemmatize_and_remove_stopwords function\n",
        "def lemmatize_and_remove_stopwords(text):\n",
        "    # Create a lemmatizer within the function\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        # Tokenize the sentence\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "        # Remove stopwords and non-alphanumeric tokens\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "\n",
        "        # Lemmatization using the local lemmatizer\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        # Convert tokens back to a single string\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "        return preprocessed_text\n",
        "    else:\n",
        "        # Handle non-string or missing values (e.g., NaN)\n",
        "        return text\n",
        "\n",
        "# Load the ICD code and description data\n",
        "icd_code_data = pd.read_csv('output.csv')\n",
        "\n",
        "# Preprocess the ICD code and description data\n",
        "icd_code_data['icd code'] = icd_code_data['icd code'].apply(lemmatize_and_remove_stopwords)\n",
        "icd_code_data['Description'] = icd_code_data['Description'].apply(lemmatize_and_remove_stopwords)"
      ],
      "metadata": {
        "id": "pnlx3rtw8chN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in the 'icd code' column with an empty string\n",
        "icd_code_data['icd code'].fillna('', inplace=True)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform the ICD code and description data into TF-IDF vectors\n",
        "X = vectorizer.fit_transform(icd_code_data['icd code'])\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, icd_code_data['Description'], test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "8nrsqPfV8hjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values from X_train and y_train\n",
        "X_train = X_train[~pd.isna(y_train)]  # Use pd.isna to check for missing values\n",
        "y_train = y_train[~pd.isna(y_train)]  # Use pd.isna to check for missing values\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4E5rqyS8lAr",
        "outputId": "e44ac049-d49d-4b59-c979-ce663c81ddfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.04487179487179487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K23G7--t8mx2",
        "outputId": "b2ec62e2-7503-49f9-8310-6ff5fcb2e8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.05048076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "y_train = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
        "y_test = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
        "\n",
        "# Create a DMatrix for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define the number of classes in your classification problem\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# Define XGBoost parameters\n",
        "params = {\n",
        "    \"objective\": \"multi:softmax\",  # for multiclass classification\n",
        "    \"num_class\": num_classes,  # number of classes\n",
        "    \"max_depth\": 3,\n",
        "    \"eta\": 0.1,\n",
        "    \"subsample\": 0.7,\n",
        "    \"colsample_bytree\": 0.7,\n",
        "    \"eval_metric\": \"mlogloss\"\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "num_round = 100\n",
        "model = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8UcRRZ98xxG",
        "outputId": "1da721c8-fbd4-4819-9d82-5dec27cb11cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 4.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "models = {\n",
        "    \"Support Vector Machine\": SVC(),\n",
        "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500)\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(f'Model: {model_name}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print('Classification Report:')\n",
        "    print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPayKbaN80zC",
        "outputId": "973c448d-d48a-4df5-f350-98bb70b07b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Support Vector Machine\n",
            "Accuracy: 0.05048076923076923\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.00      0.00      0.00       790\n",
            "         0.0       0.00      0.00      0.00        37\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "        47.0       1.00      1.00      1.00         1\n",
            "        51.0       0.00      0.00      0.00         1\n",
            "        52.0       0.00      0.00      0.00         1\n",
            "        54.0       0.00      0.00      0.00         1\n",
            "        98.0       0.00      0.00      0.00         1\n",
            "       108.0       0.00      0.00      0.00         1\n",
            "       118.0       0.00      0.00      0.00         1\n",
            "       147.0       0.00      0.00      0.00         1\n",
            "       154.0       0.00      0.00      0.00         3\n",
            "       177.0       0.00      0.00      0.00         1\n",
            "       197.0       0.00      0.00      0.00         1\n",
            "       198.0       0.00      0.00      0.00         1\n",
            "       203.0       0.00      0.00      0.00         1\n",
            "       204.0       0.00      0.00      0.00         1\n",
            "       216.0       0.00      0.00      0.00         1\n",
            "       220.0       0.00      0.00      0.00         1\n",
            "       225.0       0.00      0.00      0.00         1\n",
            "       239.0       0.00      0.00      0.00         1\n",
            "       246.0       0.00      0.00      0.00         1\n",
            "       266.0       0.00      0.00      0.00         1\n",
            "       279.0       0.00      0.00      0.00         1\n",
            "       286.0       0.00      0.00      0.00         1\n",
            "       308.0       0.00      0.00      0.00         1\n",
            "       313.0       0.00      0.00      0.00         1\n",
            "       318.0       0.00      0.00      0.00         2\n",
            "       335.0       1.00      1.00      1.00         1\n",
            "       337.0       0.00      0.00      0.00         1\n",
            "       342.0       0.00      0.00      0.00         1\n",
            "       343.0       0.00      0.00      0.00         1\n",
            "       345.0       0.00      0.00      0.00         2\n",
            "       351.0       0.00      0.00      0.00         2\n",
            "       354.0       0.00      0.00      0.00         1\n",
            "       364.0       0.00      0.00      0.00         3\n",
            "       368.0       0.00      0.00      0.00         2\n",
            "       383.0       0.00      0.00      0.00         1\n",
            "       408.0       0.00      0.00      0.00         1\n",
            "       416.0       1.00      1.00      1.00         1\n",
            "       423.0       0.00      0.00      0.00         2\n",
            "       425.0       0.00      0.00      0.00         2\n",
            "       452.0       0.00      0.00      0.00         1\n",
            "       457.0       0.00      0.00      0.00         1\n",
            "       470.0       0.00      0.00      0.00         1\n",
            "       475.0       0.00      0.00      0.00         1\n",
            "       478.0       0.00      0.00      0.00         1\n",
            "       491.0       0.00      0.00      0.00         1\n",
            "       497.0       0.00      0.00      0.00         3\n",
            "       505.0       0.00      0.00      0.00         1\n",
            "       509.0       0.00      0.00      0.00         1\n",
            "       520.0       0.00      0.00      0.00         1\n",
            "       537.0       0.00      0.00      0.00         1\n",
            "       553.0       0.00      0.00      0.00         2\n",
            "       560.0       0.00      0.00      0.00         1\n",
            "       580.0       0.00      0.00      0.00         2\n",
            "       584.0       0.00      0.00      0.00         1\n",
            "       588.0       0.00      0.00      0.00         1\n",
            "       592.0       1.00      1.00      1.00         1\n",
            "       614.0       0.00      0.00      0.00         1\n",
            "       620.0       0.00      0.00      0.00         2\n",
            "       628.0       0.00      0.00      0.00         1\n",
            "       647.0       0.00      0.00      0.00         1\n",
            "       657.0       0.00      0.00      0.00         1\n",
            "       667.0       0.00      0.00      0.00         1\n",
            "       672.0       0.00      0.00      0.00         1\n",
            "       680.0       0.00      0.00      0.00         1\n",
            "       695.0       0.00      0.00      0.00         1\n",
            "       701.0       0.00      0.00      0.00         1\n",
            "       707.0       0.00      0.00      0.00         1\n",
            "       722.0       1.00      1.00      1.00         1\n",
            "       725.0       0.00      0.00      0.00         1\n",
            "       727.0       0.00      0.00      0.00         1\n",
            "       738.0       0.00      0.00      0.00         1\n",
            "       744.0       0.00      0.00      0.00         1\n",
            "       750.0       0.00      0.00      0.00         1\n",
            "       751.0       0.00      0.00      0.00         3\n",
            "       769.0       0.00      0.00      0.00         1\n",
            "       785.0       0.00      0.00      0.00         1\n",
            "       813.0       0.00      0.00      0.00         1\n",
            "       823.0       0.00      0.00      0.00         1\n",
            "       844.0       0.00      0.00      0.00         1\n",
            "       900.0       0.00      0.00      0.00         1\n",
            "       902.0       0.00      0.00      0.00         2\n",
            "       903.0       0.00      0.00      0.00         2\n",
            "       904.0       0.00      0.00      0.00         1\n",
            "       909.0       0.00      0.00      0.00         1\n",
            "       920.0       0.00      0.00      0.00         1\n",
            "       921.0       0.00      0.00      0.00         2\n",
            "       922.0       0.00      0.00      0.00         1\n",
            "       932.0       0.00      0.00      0.00         1\n",
            "       934.0       0.00      0.00      0.00         1\n",
            "       936.0       0.00      0.00      0.00         1\n",
            "       951.0       0.00      0.00      0.00         1\n",
            "       971.0       0.00      0.00      0.00         1\n",
            "       978.0       0.00      0.00      0.00         1\n",
            "       991.0       0.00      0.00      0.00         1\n",
            "       992.0       0.00      0.00      0.00         1\n",
            "      1010.0       0.00      0.00      0.00         3\n",
            "      1020.0       0.00      0.00      0.00         1\n",
            "      1047.0       0.00      0.00      0.00         1\n",
            "      1063.0       0.00      0.00      0.00         1\n",
            "      1070.0       0.00      0.00      0.00         1\n",
            "      1086.0       0.00      0.00      0.00         1\n",
            "      1106.0       0.00      0.00      0.00         1\n",
            "      1108.0       0.00      0.00      0.00         2\n",
            "      1116.0       0.00      0.00      0.00         1\n",
            "      1117.0       0.00      0.00      0.00         5\n",
            "      1136.0       0.00      0.00      0.00         1\n",
            "      1157.0       0.00      0.00      0.00         1\n",
            "      1161.0       0.00      0.00      0.00         1\n",
            "      1202.0       0.00      0.00      0.00         1\n",
            "      1213.0       0.00      0.00      0.00         1\n",
            "      1232.0       0.00      0.00      0.00         2\n",
            "      1233.0       0.00      0.00      0.00         1\n",
            "      1251.0       0.00      0.00      0.00         1\n",
            "      1257.0       0.00      0.00      0.00         2\n",
            "      1260.0       0.00      0.00      0.00         1\n",
            "      1269.0       0.00      0.00      0.00         1\n",
            "      1279.0       0.00      0.00      0.00         1\n",
            "      1305.0       0.00      0.00      0.00         1\n",
            "      1311.0       0.00      0.00      0.00         0\n",
            "      1323.0       0.00      0.00      0.00         1\n",
            "      1325.0       0.00      0.00      0.00         2\n",
            "      1343.0       0.00      0.00      0.00         1\n",
            "      1360.0       0.00      0.00      0.00         1\n",
            "      1374.0       0.00      0.00      0.00         2\n",
            "      1378.0       0.00      0.00      0.00         2\n",
            "      1379.0       0.00      0.00      0.00         1\n",
            "      1386.0       0.00      0.00      0.00         1\n",
            "      1391.0       0.00      0.00      0.00         1\n",
            "      1402.0       0.00      0.00      0.00         1\n",
            "      1405.0       0.00      0.00      0.00         1\n",
            "      1409.0       0.00      0.00      0.00         2\n",
            "      1419.0       0.00      0.00      0.00         3\n",
            "      1420.0       0.00      0.00      0.00         1\n",
            "      1430.0       0.00      0.00      0.00         1\n",
            "      1432.0       0.00      0.00      0.00         3\n",
            "      1436.0       0.00      0.00      0.00         2\n",
            "      1455.0       0.00      0.00      0.00         1\n",
            "      1456.0       0.00      0.00      0.00         2\n",
            "      1465.0       0.00      0.00      0.00         1\n",
            "      1467.0       0.00      0.00      0.00         1\n",
            "      1481.0       0.00      0.00      0.00         1\n",
            "      1547.0       0.00      0.00      0.00         1\n",
            "      1548.0       0.00      0.00      0.00         1\n",
            "      1556.0       0.00      0.00      0.00         1\n",
            "      1564.0       0.00      0.00      0.00         1\n",
            "      1568.0       0.00      0.00      0.00         1\n",
            "      1571.0       0.00      0.00      0.00         1\n",
            "      1578.0       0.00      0.00      0.00         1\n",
            "      1585.0       0.00      0.00      0.00         1\n",
            "      1638.0       0.00      0.00      0.00         1\n",
            "      1639.0       0.00      0.00      0.00         2\n",
            "      1642.0       0.00      0.00      0.00         1\n",
            "      1646.0       1.00      1.00      1.00         1\n",
            "      1663.0       0.00      0.00      0.00         1\n",
            "      1687.0       0.00      0.00      0.00         1\n",
            "      1691.0       1.00      1.00      1.00         1\n",
            "      1696.0       0.00      0.00      0.00         1\n",
            "      1703.0       0.00      0.00      0.00         4\n",
            "      1705.0       0.00      0.00      0.00         3\n",
            "      1728.0       0.00      0.00      0.00         1\n",
            "      1729.0       0.00      0.00      0.00         2\n",
            "      1741.0       0.00      0.00      0.00         1\n",
            "      1765.0       0.00      0.00      0.00         3\n",
            "      1772.0       0.00      0.00      0.00         1\n",
            "      1781.0       0.00      0.00      0.00         1\n",
            "      1795.0       0.00      0.00      0.00         2\n",
            "      1804.0       0.00      0.00      0.00         3\n",
            "      1811.0       0.00      0.00      0.00         3\n",
            "      1821.0       0.00      0.00      0.00         1\n",
            "      1844.0       0.00      0.00      0.00         1\n",
            "      1851.0       0.00      0.00      0.00         1\n",
            "      1862.0       0.00      0.00      0.00         1\n",
            "      1864.0       0.00      0.00      0.00         1\n",
            "      1865.0       0.00      0.00      0.00         2\n",
            "      1869.0       0.00      0.00      0.00         1\n",
            "      1875.0       0.00      0.00      0.00         1\n",
            "      1899.0       0.00      0.00      0.00         1\n",
            "      1927.0       0.00      0.00      0.00         1\n",
            "      1933.0       0.00      0.00      0.00         3\n",
            "      1959.0       0.00      0.00      0.00         1\n",
            "      1962.0       0.00      0.00      0.00         1\n",
            "      1964.0       0.00      0.00      0.00         1\n",
            "      1975.0       0.00      0.00      0.00         1\n",
            "      1982.0       0.00      0.00      0.00         1\n",
            "      1986.0       0.00      0.00      0.00         1\n",
            "      1994.0       0.00      0.00      0.00         2\n",
            "      1996.0       0.00      0.00      0.00         1\n",
            "      2022.0       0.00      0.00      0.00         1\n",
            "      2028.0       0.00      0.00      0.00         2\n",
            "      2057.0       0.00      0.00      0.00         1\n",
            "      2066.0       0.00      0.00      0.00         1\n",
            "      2068.0       0.00      0.00      0.00         1\n",
            "      2072.0       0.00      0.00      0.00         5\n",
            "      2095.0       0.00      0.00      0.00         1\n",
            "      2114.0       0.00      0.00      0.00         1\n",
            "      2119.0       0.00      0.00      0.00         1\n",
            "      2142.0       0.00      0.00      0.00         1\n",
            "      2143.0       0.00      0.00      0.00         9\n",
            "      2175.0       0.00      0.00      0.00         7\n",
            "      2177.0       0.00      0.00      0.00         1\n",
            "      2178.0       0.00      0.00      0.00         2\n",
            "      2184.0       0.00      0.00      0.00         1\n",
            "      2210.0       0.00      0.00      0.00         1\n",
            "      2212.0       0.00      0.00      0.00         1\n",
            "      2223.0       0.00      0.00      0.00         1\n",
            "      2232.0       0.00      0.00      0.00         1\n",
            "      2251.0       0.00      0.00      0.00         1\n",
            "      2258.0       0.00      0.00      0.00         1\n",
            "      2280.0       0.00      0.00      0.00         1\n",
            "      2281.0       0.00      0.00      0.00         1\n",
            "      2286.0       0.00      0.00      0.00         3\n",
            "      2293.0       0.00      0.00      0.00         4\n",
            "      2301.0       0.00      0.00      0.00         1\n",
            "      2311.0       0.00      0.00      0.00         3\n",
            "      2313.0       0.00      0.00      0.00         1\n",
            "      2321.0       0.00      0.00      0.00         1\n",
            "      2322.0       0.00      0.00      0.00         3\n",
            "      2323.0       0.00      0.00      0.00         1\n",
            "      2351.0       0.00      0.00      0.00         3\n",
            "      2353.0       0.00      0.00      0.00         1\n",
            "      2356.0       0.00      0.00      0.00         1\n",
            "      2359.0       0.00      0.00      0.00         1\n",
            "      2362.0       0.00      0.00      0.00         1\n",
            "      2378.0       0.00      0.00      0.00         1\n",
            "      2385.0       0.00      0.00      0.00         2\n",
            "      2391.0       0.00      0.00      0.00         1\n",
            "      2394.0       0.00      0.00      0.00         1\n",
            "      2398.0       0.00      0.00      0.00         1\n",
            "      2405.0       0.00      0.00      0.00         1\n",
            "      2410.0       0.00      0.00      0.00         1\n",
            "      2414.0       0.00      0.00      0.00         1\n",
            "      2415.0       0.00      0.00      0.00         1\n",
            "      2417.0       0.00      0.00      0.00         1\n",
            "      2430.0       0.00      0.00      0.00         2\n",
            "      2449.0       0.00      0.00      0.00         1\n",
            "      2474.0       0.00      0.00      0.00         1\n",
            "      2478.0       0.00      0.00      0.00         1\n",
            "      2479.0       0.05      1.00      0.10        56\n",
            "      2487.0       0.00      0.00      0.00         1\n",
            "      2488.0       0.00      0.00      0.00         2\n",
            "      2503.0       0.00      0.00      0.00         1\n",
            "      2507.0       0.00      0.00      0.00         1\n",
            "      2509.0       0.00      0.00      0.00         1\n",
            "      2510.0       0.00      0.00      0.00         1\n",
            "      2511.0       0.00      0.00      0.00         1\n",
            "      2513.0       0.00      0.00      0.00         2\n",
            "      2532.0       0.00      0.00      0.00         1\n",
            "      2551.0       0.00      0.00      0.00         1\n",
            "      2557.0       0.00      0.00      0.00         2\n",
            "      2565.0       0.00      0.00      0.00         1\n",
            "      2569.0       0.00      0.00      0.00         3\n",
            "      2588.0       0.00      0.00      0.00         2\n",
            "      2590.0       0.00      0.00      0.00         1\n",
            "      2608.0       0.00      0.00      0.00         1\n",
            "      2609.0       0.00      0.00      0.00         1\n",
            "      2620.0       0.00      0.00      0.00         2\n",
            "      2621.0       0.00      0.00      0.00         1\n",
            "      2644.0       0.00      0.00      0.00         3\n",
            "      2661.0       0.00      0.00      0.00         1\n",
            "      2663.0       0.00      0.00      0.00         1\n",
            "      2664.0       0.00      0.00      0.00         1\n",
            "      2678.0       0.00      0.00      0.00         1\n",
            "      2703.0       0.00      0.00      0.00         1\n",
            "      2708.0       0.00      0.00      0.00         1\n",
            "      2712.0       0.00      0.00      0.00         1\n",
            "      2716.0       0.00      0.00      0.00         1\n",
            "      2721.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.05      1248\n",
            "   macro avg       0.03      0.03      0.03      1248\n",
            "weighted avg       0.01      0.05      0.01      1248\n",
            "\n",
            "Model: Neural Network\n",
            "Accuracy: 0.05048076923076923\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.00      0.00      0.00       790\n",
            "         0.0       0.00      0.00      0.00        37\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "        47.0       1.00      1.00      1.00         1\n",
            "        51.0       0.00      0.00      0.00         1\n",
            "        52.0       0.00      0.00      0.00         1\n",
            "        54.0       0.00      0.00      0.00         1\n",
            "        85.0       0.00      0.00      0.00         0\n",
            "        98.0       0.00      0.00      0.00         1\n",
            "       108.0       0.00      0.00      0.00         1\n",
            "       118.0       0.00      0.00      0.00         1\n",
            "       147.0       0.00      0.00      0.00         1\n",
            "       154.0       0.00      0.00      0.00         3\n",
            "       177.0       0.00      0.00      0.00         1\n",
            "       197.0       0.00      0.00      0.00         1\n",
            "       198.0       0.00      0.00      0.00         1\n",
            "       203.0       0.00      0.00      0.00         1\n",
            "       204.0       0.00      0.00      0.00         1\n",
            "       216.0       0.00      0.00      0.00         1\n",
            "       220.0       0.00      0.00      0.00         1\n",
            "       225.0       0.00      0.00      0.00         1\n",
            "       239.0       0.00      0.00      0.00         1\n",
            "       246.0       0.00      0.00      0.00         1\n",
            "       266.0       0.00      0.00      0.00         1\n",
            "       279.0       0.00      0.00      0.00         1\n",
            "       286.0       0.00      0.00      0.00         1\n",
            "       308.0       0.00      0.00      0.00         1\n",
            "       313.0       0.00      0.00      0.00         1\n",
            "       318.0       0.00      0.00      0.00         2\n",
            "       335.0       1.00      1.00      1.00         1\n",
            "       337.0       0.00      0.00      0.00         1\n",
            "       342.0       0.00      0.00      0.00         1\n",
            "       343.0       0.00      0.00      0.00         1\n",
            "       345.0       0.00      0.00      0.00         2\n",
            "       351.0       0.00      0.00      0.00         2\n",
            "       354.0       0.00      0.00      0.00         1\n",
            "       364.0       0.00      0.00      0.00         3\n",
            "       368.0       0.00      0.00      0.00         2\n",
            "       383.0       0.00      0.00      0.00         1\n",
            "       408.0       0.00      0.00      0.00         1\n",
            "       416.0       1.00      1.00      1.00         1\n",
            "       423.0       0.00      0.00      0.00         2\n",
            "       425.0       0.00      0.00      0.00         2\n",
            "       452.0       0.00      0.00      0.00         1\n",
            "       457.0       0.00      0.00      0.00         1\n",
            "       470.0       0.00      0.00      0.00         1\n",
            "       475.0       0.00      0.00      0.00         1\n",
            "       478.0       0.00      0.00      0.00         1\n",
            "       491.0       0.00      0.00      0.00         1\n",
            "       497.0       0.00      0.00      0.00         3\n",
            "       505.0       0.00      0.00      0.00         1\n",
            "       509.0       0.00      0.00      0.00         1\n",
            "       520.0       0.00      0.00      0.00         1\n",
            "       537.0       0.00      0.00      0.00         1\n",
            "       553.0       0.00      0.00      0.00         2\n",
            "       560.0       0.00      0.00      0.00         1\n",
            "       580.0       0.00      0.00      0.00         2\n",
            "       584.0       0.00      0.00      0.00         1\n",
            "       588.0       0.00      0.00      0.00         1\n",
            "       592.0       1.00      1.00      1.00         1\n",
            "       614.0       0.00      0.00      0.00         1\n",
            "       620.0       0.00      0.00      0.00         2\n",
            "       628.0       0.00      0.00      0.00         1\n",
            "       647.0       0.00      0.00      0.00         1\n",
            "       657.0       0.00      0.00      0.00         1\n",
            "       667.0       0.00      0.00      0.00         1\n",
            "       672.0       0.00      0.00      0.00         1\n",
            "       680.0       0.00      0.00      0.00         1\n",
            "       695.0       0.00      0.00      0.00         1\n",
            "       701.0       0.00      0.00      0.00         1\n",
            "       707.0       0.00      0.00      0.00         1\n",
            "       722.0       1.00      1.00      1.00         1\n",
            "       725.0       0.00      0.00      0.00         1\n",
            "       727.0       0.00      0.00      0.00         1\n",
            "       738.0       0.00      0.00      0.00         1\n",
            "       744.0       0.00      0.00      0.00         1\n",
            "       750.0       0.00      0.00      0.00         1\n",
            "       751.0       0.00      0.00      0.00         3\n",
            "       769.0       0.00      0.00      0.00         1\n",
            "       785.0       0.00      0.00      0.00         1\n",
            "       813.0       0.00      0.00      0.00         1\n",
            "       823.0       0.00      0.00      0.00         1\n",
            "       844.0       0.00      0.00      0.00         1\n",
            "       900.0       0.00      0.00      0.00         1\n",
            "       902.0       0.00      0.00      0.00         2\n",
            "       903.0       0.00      0.00      0.00         2\n",
            "       904.0       0.00      0.00      0.00         1\n",
            "       909.0       0.00      0.00      0.00         1\n",
            "       920.0       0.00      0.00      0.00         1\n",
            "       921.0       0.00      0.00      0.00         2\n",
            "       922.0       0.00      0.00      0.00         1\n",
            "       932.0       0.00      0.00      0.00         1\n",
            "       934.0       0.00      0.00      0.00         1\n",
            "       936.0       0.00      0.00      0.00         1\n",
            "       951.0       0.00      0.00      0.00         1\n",
            "       971.0       0.00      0.00      0.00         1\n",
            "       978.0       0.00      0.00      0.00         1\n",
            "       991.0       0.00      0.00      0.00         1\n",
            "       992.0       0.00      0.00      0.00         1\n",
            "      1010.0       0.00      0.00      0.00         3\n",
            "      1020.0       0.00      0.00      0.00         1\n",
            "      1047.0       0.00      0.00      0.00         1\n",
            "      1063.0       0.00      0.00      0.00         1\n",
            "      1070.0       0.00      0.00      0.00         1\n",
            "      1086.0       0.00      0.00      0.00         1\n",
            "      1106.0       0.00      0.00      0.00         1\n",
            "      1108.0       0.00      0.00      0.00         2\n",
            "      1116.0       0.00      0.00      0.00         1\n",
            "      1117.0       0.00      0.00      0.00         5\n",
            "      1136.0       0.00      0.00      0.00         1\n",
            "      1157.0       0.00      0.00      0.00         1\n",
            "      1161.0       0.00      0.00      0.00         1\n",
            "      1202.0       0.00      0.00      0.00         1\n",
            "      1213.0       0.00      0.00      0.00         1\n",
            "      1232.0       0.00      0.00      0.00         2\n",
            "      1233.0       0.00      0.00      0.00         1\n",
            "      1251.0       0.00      0.00      0.00         1\n",
            "      1257.0       0.00      0.00      0.00         2\n",
            "      1260.0       0.00      0.00      0.00         1\n",
            "      1269.0       0.00      0.00      0.00         1\n",
            "      1279.0       0.00      0.00      0.00         1\n",
            "      1305.0       0.00      0.00      0.00         1\n",
            "      1323.0       0.00      0.00      0.00         1\n",
            "      1325.0       0.00      0.00      0.00         2\n",
            "      1343.0       0.00      0.00      0.00         1\n",
            "      1360.0       0.00      0.00      0.00         1\n",
            "      1374.0       0.00      0.00      0.00         2\n",
            "      1378.0       0.00      0.00      0.00         2\n",
            "      1379.0       0.00      0.00      0.00         1\n",
            "      1386.0       0.00      0.00      0.00         1\n",
            "      1391.0       0.00      0.00      0.00         1\n",
            "      1402.0       0.00      0.00      0.00         1\n",
            "      1405.0       0.00      0.00      0.00         1\n",
            "      1409.0       0.00      0.00      0.00         2\n",
            "      1419.0       0.00      0.00      0.00         3\n",
            "      1420.0       0.00      0.00      0.00         1\n",
            "      1430.0       0.00      0.00      0.00         1\n",
            "      1432.0       0.00      0.00      0.00         3\n",
            "      1436.0       0.00      0.00      0.00         2\n",
            "      1455.0       0.00      0.00      0.00         1\n",
            "      1456.0       0.00      0.00      0.00         2\n",
            "      1465.0       0.00      0.00      0.00         1\n",
            "      1467.0       0.00      0.00      0.00         1\n",
            "      1481.0       0.00      0.00      0.00         1\n",
            "      1547.0       0.00      0.00      0.00         1\n",
            "      1548.0       0.00      0.00      0.00         1\n",
            "      1556.0       0.00      0.00      0.00         1\n",
            "      1564.0       0.00      0.00      0.00         1\n",
            "      1568.0       0.00      0.00      0.00         1\n",
            "      1571.0       0.00      0.00      0.00         1\n",
            "      1578.0       0.00      0.00      0.00         1\n",
            "      1585.0       0.00      0.00      0.00         1\n",
            "      1638.0       0.00      0.00      0.00         1\n",
            "      1639.0       0.00      0.00      0.00         2\n",
            "      1642.0       0.00      0.00      0.00         1\n",
            "      1646.0       1.00      1.00      1.00         1\n",
            "      1663.0       0.00      0.00      0.00         1\n",
            "      1687.0       0.00      0.00      0.00         1\n",
            "      1691.0       1.00      1.00      1.00         1\n",
            "      1696.0       0.00      0.00      0.00         1\n",
            "      1703.0       0.00      0.00      0.00         4\n",
            "      1705.0       0.00      0.00      0.00         3\n",
            "      1728.0       0.00      0.00      0.00         1\n",
            "      1729.0       0.00      0.00      0.00         2\n",
            "      1741.0       0.00      0.00      0.00         1\n",
            "      1765.0       0.00      0.00      0.00         3\n",
            "      1772.0       0.00      0.00      0.00         1\n",
            "      1781.0       0.00      0.00      0.00         1\n",
            "      1795.0       0.00      0.00      0.00         2\n",
            "      1804.0       0.00      0.00      0.00         3\n",
            "      1811.0       0.00      0.00      0.00         3\n",
            "      1821.0       0.00      0.00      0.00         1\n",
            "      1844.0       0.00      0.00      0.00         1\n",
            "      1851.0       0.00      0.00      0.00         1\n",
            "      1862.0       0.00      0.00      0.00         1\n",
            "      1864.0       0.00      0.00      0.00         1\n",
            "      1865.0       0.00      0.00      0.00         2\n",
            "      1869.0       0.00      0.00      0.00         1\n",
            "      1875.0       0.00      0.00      0.00         1\n",
            "      1899.0       0.00      0.00      0.00         1\n",
            "      1927.0       0.00      0.00      0.00         1\n",
            "      1933.0       0.00      0.00      0.00         3\n",
            "      1959.0       0.00      0.00      0.00         1\n",
            "      1962.0       0.00      0.00      0.00         1\n",
            "      1964.0       0.00      0.00      0.00         1\n",
            "      1975.0       0.00      0.00      0.00         1\n",
            "      1982.0       0.00      0.00      0.00         1\n",
            "      1986.0       0.00      0.00      0.00         1\n",
            "      1994.0       0.00      0.00      0.00         2\n",
            "      1996.0       0.00      0.00      0.00         1\n",
            "      2022.0       0.00      0.00      0.00         1\n",
            "      2028.0       0.00      0.00      0.00         2\n",
            "      2057.0       0.00      0.00      0.00         1\n",
            "      2066.0       0.00      0.00      0.00         1\n",
            "      2068.0       0.00      0.00      0.00         1\n",
            "      2072.0       0.00      0.00      0.00         5\n",
            "      2095.0       0.00      0.00      0.00         1\n",
            "      2114.0       0.00      0.00      0.00         1\n",
            "      2119.0       0.00      0.00      0.00         1\n",
            "      2142.0       0.00      0.00      0.00         1\n",
            "      2143.0       0.00      0.00      0.00         9\n",
            "      2175.0       0.00      0.00      0.00         7\n",
            "      2177.0       0.00      0.00      0.00         1\n",
            "      2178.0       0.00      0.00      0.00         2\n",
            "      2184.0       0.00      0.00      0.00         1\n",
            "      2210.0       0.00      0.00      0.00         1\n",
            "      2212.0       0.00      0.00      0.00         1\n",
            "      2223.0       0.00      0.00      0.00         1\n",
            "      2232.0       0.00      0.00      0.00         1\n",
            "      2251.0       0.00      0.00      0.00         1\n",
            "      2258.0       0.00      0.00      0.00         1\n",
            "      2280.0       0.00      0.00      0.00         1\n",
            "      2281.0       0.00      0.00      0.00         1\n",
            "      2286.0       0.00      0.00      0.00         3\n",
            "      2293.0       0.00      0.00      0.00         4\n",
            "      2301.0       0.00      0.00      0.00         1\n",
            "      2311.0       0.00      0.00      0.00         3\n",
            "      2313.0       0.00      0.00      0.00         1\n",
            "      2321.0       0.00      0.00      0.00         1\n",
            "      2322.0       0.00      0.00      0.00         3\n",
            "      2323.0       0.00      0.00      0.00         1\n",
            "      2351.0       0.00      0.00      0.00         3\n",
            "      2353.0       0.00      0.00      0.00         1\n",
            "      2356.0       0.00      0.00      0.00         1\n",
            "      2359.0       0.00      0.00      0.00         1\n",
            "      2362.0       0.00      0.00      0.00         1\n",
            "      2378.0       0.00      0.00      0.00         1\n",
            "      2385.0       0.00      0.00      0.00         2\n",
            "      2391.0       0.00      0.00      0.00         1\n",
            "      2394.0       0.00      0.00      0.00         1\n",
            "      2398.0       0.00      0.00      0.00         1\n",
            "      2405.0       0.00      0.00      0.00         1\n",
            "      2410.0       0.00      0.00      0.00         1\n",
            "      2414.0       0.00      0.00      0.00         1\n",
            "      2415.0       0.00      0.00      0.00         1\n",
            "      2417.0       0.00      0.00      0.00         1\n",
            "      2430.0       0.00      0.00      0.00         2\n",
            "      2449.0       0.00      0.00      0.00         1\n",
            "      2474.0       0.00      0.00      0.00         1\n",
            "      2478.0       0.00      0.00      0.00         1\n",
            "      2479.0       0.05      1.00      0.09        56\n",
            "      2487.0       0.00      0.00      0.00         1\n",
            "      2488.0       0.00      0.00      0.00         2\n",
            "      2503.0       0.00      0.00      0.00         1\n",
            "      2507.0       0.00      0.00      0.00         1\n",
            "      2509.0       0.00      0.00      0.00         1\n",
            "      2510.0       0.00      0.00      0.00         1\n",
            "      2511.0       0.00      0.00      0.00         1\n",
            "      2513.0       0.00      0.00      0.00         2\n",
            "      2532.0       0.00      0.00      0.00         1\n",
            "      2551.0       0.00      0.00      0.00         1\n",
            "      2557.0       0.00      0.00      0.00         2\n",
            "      2565.0       0.00      0.00      0.00         1\n",
            "      2569.0       0.00      0.00      0.00         3\n",
            "      2588.0       0.00      0.00      0.00         2\n",
            "      2590.0       0.00      0.00      0.00         1\n",
            "      2608.0       0.00      0.00      0.00         1\n",
            "      2609.0       0.00      0.00      0.00         1\n",
            "      2620.0       0.00      0.00      0.00         2\n",
            "      2621.0       0.00      0.00      0.00         1\n",
            "      2644.0       0.00      0.00      0.00         3\n",
            "      2661.0       0.00      0.00      0.00         1\n",
            "      2663.0       0.00      0.00      0.00         1\n",
            "      2664.0       0.00      0.00      0.00         1\n",
            "      2678.0       0.00      0.00      0.00         1\n",
            "      2703.0       0.00      0.00      0.00         1\n",
            "      2708.0       0.00      0.00      0.00         1\n",
            "      2712.0       0.00      0.00      0.00         1\n",
            "      2716.0       0.00      0.00      0.00         1\n",
            "      2721.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.05      1248\n",
            "   macro avg       0.03      0.03      0.03      1248\n",
            "weighted avg       0.01      0.05      0.01      1248\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExOblVMX-o5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}